# ğŸ¢ ì—°êµ¬ì‹¤ ì„œë²„ LM Studio ì„¤ì¹˜ ê°€ì´ë“œ

## ğŸ“‹ ì‚¬ì „ ì¤€ë¹„ ì‚¬í•­

### ì„œë²„ ìŠ¤í™ í™•ì¸
```bash
# GPU í™•ì¸
nvidia-smi
# ë˜ëŠ” Macì˜ ê²½ìš°
system_profiler SPDisplaysDataType

# RAM í™•ì¸  
free -h  # Linux
# ë˜ëŠ” Macì˜ ê²½ìš°
sysctl hw.memsize

# ë””ìŠ¤í¬ ê³µê°„ í™•ì¸ (ìµœì†Œ 10GB í•„ìš”)
df -h
```

### í•„ìš” ìµœì†Œ ì‚¬ì–‘
- **GPU**: RTX 3060 (6GB) ì´ìƒ ë˜ëŠ” M1/M2 Mac
- **RAM**: 8GB ì´ìƒ (16GB ê¶Œì¥)
- **Storage**: 10GB ì—¬ìœ  ê³µê°„
- **Network**: ì•ˆì •ì ì¸ ì¸í„°ë„· ì—°ê²°

## ğŸš€ ì„¤ì¹˜ ë°©ë²•

### Option A: GUI ë²„ì „ (ì¶”ì²œ)

#### Windows/Mac ì„œë²„
```bash
# 1. LM Studio ë‹¤ìš´ë¡œë“œ
# https://lmstudio.ai

# 2. ì„¤ì¹˜ í›„ ì‹¤í–‰

# 3. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
# Searchì—ì„œ "google/gemma-2-3b-it-GGUF" ê²€ìƒ‰
# "gemma-2-3b-it-q4_k_m.gguf" ë‹¤ìš´ë¡œë“œ (ì•½ 2GB)

# 4. ì„œë²„ ì„¤ì •
# Server íƒ­ ì´ë™
# Model: google/gemma-2-3b-it-q4_k_m.gguf ì„ íƒ
# Enable CORS: âœ…
# Allow Remote Connections: âœ…
# Port: 1234 (ê¸°ë³¸ê°’)
# Start Server í´ë¦­
```

### Option B: Docker ë²„ì „ (Linux ì„œë²„)

#### Ubuntu/CentOS
```bash
# 1. Docker ì„¤ì¹˜
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh

# 2. NVIDIA Docker ì„¤ì¹˜ (GPU ì‚¬ìš©ì‹œ)
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt-get update && sudo apt-get install -y nvidia-docker2
sudo systemctl restart docker

# 3. LM Studio ì»¨í…Œì´ë„ˆ ì‹¤í–‰
docker run -d \
  --name lm-studio-server \
  --gpus all \
  -p 1234:1234 \
  -v ~/lm-studio-models:/app/models \
  --restart unless-stopped \
  lmstudio/server:latest

# 4. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
docker exec -it lm-studio-server bash
# ì»¨í…Œì´ë„ˆ ë‚´ì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
```

### Option C: ìˆ˜ë™ ì„¤ì¹˜ (ê³ ê¸‰)

#### Python í™˜ê²½ êµ¬ì¶•
```bash
# 1. Conda í™˜ê²½ ìƒì„±
conda create -n lmstudio python=3.10
conda activate lmstudio

# 2. í•„ìš” íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install transformers accelerate bitsandbytes
pip install fastapi uvicorn

# 3. ì„œë²„ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„± (ë³„ë„ íŒŒì¼ ìƒì„±)
```

## ğŸŒ ë„¤íŠ¸ì›Œí¬ ì„¤ì •

### ë°©í™”ë²½ ì„¤ì •
```bash
# Ubuntu/Debian
sudo ufw allow 1234

# CentOS/RHEL
sudo firewall-cmd --add-port=1234/tcp --permanent
sudo firewall-cmd --reload

# Windows
# Windows Defender ë°©í™”ë²½ì—ì„œ 1234 í¬íŠ¸ ì—´ê¸°
```

### ê³ ì • IP ì„¤ì • (ì„ íƒì‚¬í•­)
```bash
# ì—°êµ¬ì‹¤ ë‚´ë¶€ ê³ ì • IP ì„¤ì •
# ì˜ˆ: 192.168.1.100

# Ubuntu netplan ì„¤ì •
sudo nano /etc/netplan/01-network-manager-all.yaml

# ë‚´ìš©:
network:
  version: 2
  ethernets:
    eth0:
      dhcp4: false
      addresses: [192.168.1.100/24]
      gateway4: 192.168.1.1
      nameservers:
        addresses: [8.8.8.8, 8.8.4.4]

sudo netplan apply
```

## ğŸ”§ ì›ê²© ì ‘ê·¼ ì„¤ì •

### SSH í„°ë„ë§ (ë³´ì•ˆ ê°•í™”)
```bash
# ë¡œì»¬ ì»´í“¨í„°ì—ì„œ ì—°êµ¬ì‹¤ ì„œë²„ë¡œ SSH í„°ë„
ssh -L 1234:localhost:1234 username@lab-server-ip

# ê·¸ëŸ¬ë©´ localhost:1234ë¡œ ì ‘ê·¼ ê°€ëŠ¥
```

### VPN ì„¤ì • (ê¶Œì¥)
```bash
# ì—°êµ¬ì‹¤ VPN í†µí•´ì„œë§Œ ì ‘ê·¼
# ë³´ì•ˆì„± â†‘, ì„¤ì • ë³µì¡ë„ â†‘
```

### ì§ì ‘ ì ‘ê·¼ (ê°„ë‹¨í•¨)
```bash
# .env.local ìˆ˜ì •
LMSTUDIO_URL=http://LAB-SERVER-IP:1234/v1

# ì˜ˆì‹œ
LMSTUDIO_URL=http://192.168.1.100:1234/v1
```

## ğŸ“Š ì„±ëŠ¥ ìµœì í™”

### GPU ë©”ëª¨ë¦¬ ìµœì í™”
```bash
# LM Studio ì„¤ì •ì—ì„œ
# GPU Layers: ìë™ ë˜ëŠ” 20-30 (RTX 3060 ê¸°ì¤€)
# Context Length: 2048 (ê¸°ë³¸ê°’)
# Batch Size: 4-8
```

### ì‹œìŠ¤í…œ ìµœì í™”
```bash
# Linux ìŠ¤ì™‘ ë¹„í™œì„±í™” (ì„±ëŠ¥ í–¥ìƒ)
sudo swapoff -a

# GPU ì„±ëŠ¥ ëª¨ë“œ ì„¤ì •
sudo nvidia-smi -pm 1
sudo nvidia-smi -pl 300  # ì „ë ¥ ì œí•œ 300W
```

## ğŸ” ëª¨ë‹ˆí„°ë§ ì„¤ì •

### ì„œë²„ ìƒíƒœ ëª¨ë‹ˆí„°ë§
```bash
# ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
htop
nvidia-smi -l 1  # GPU ìƒíƒœ ì‹¤ì‹œê°„ í™•ì¸

# LM Studio í”„ë¡œì„¸ìŠ¤ í™•ì¸
ps aux | grep lmstudio
netstat -tlnp | grep 1234
```

### ìë™ ì¬ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸
```bash
#!/bin/bash
# restart-lmstudio.sh

while true; do
    if ! curl -f http://localhost:1234/v1/models > /dev/null 2>&1; then
        echo "LM Studioê°€ ì‘ë‹µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì¬ì‹œì‘í•©ë‹ˆë‹¤..."
        # Docker ì¬ì‹œì‘
        docker restart lm-studio-server
        # ë˜ëŠ” ì„œë¹„ìŠ¤ ì¬ì‹œì‘
        # systemctl restart lmstudio
        
        sleep 30
    fi
    sleep 60
done
```

### systemd ì„œë¹„ìŠ¤ ë“±ë¡ (ìë™ ì‹œì‘)
```bash
# /etc/systemd/system/lmstudio.service
[Unit]
Description=LM Studio Server
After=network.target

[Service]
Type=simple
User=username
WorkingDirectory=/home/username
ExecStart=/usr/bin/docker start -a lm-studio-server
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target

# ì„œë¹„ìŠ¤ ë“±ë¡ ë° ì‹œì‘
sudo systemctl enable lmstudio
sudo systemctl start lmstudio
```

## âœ… ì„¤ì¹˜ ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] ì„œë²„ ìŠ¤í™ í™•ì¸ ì™„ë£Œ
- [ ] LM Studio ì„¤ì¹˜ ì™„ë£Œ
- [ ] Gemma-2-3b ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ
- [ ] ì„œë²„ ì‹¤í–‰ ë° í¬íŠ¸ 1234 ì˜¤í”ˆ
- [ ] ë°©í™”ë²½ ì„¤ì • ì™„ë£Œ
- [ ] ì›ê²© ì ‘ê·¼ í…ŒìŠ¤íŠ¸ ì™„ë£Œ
- [ ] ì„±ëŠ¥ ìµœì í™” ì„¤ì • ì™„ë£Œ
- [ ] ëª¨ë‹ˆí„°ë§ ë„êµ¬ ì„¤ì • ì™„ë£Œ
- [ ] ìë™ ì¬ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸ ì„¤ì • ì™„ë£Œ

## ğŸ”„ ì›¹ì•± ì—°ê²°

### í™˜ê²½ë³€ìˆ˜ ì—…ë°ì´íŠ¸
```bash
# .env.local ìˆ˜ì •
AI_PROVIDER=lmstudio
LMSTUDIO_URL=http://LAB-SERVER-IP:1234/v1
LMSTUDIO_MODEL=google/gemma-2-3b-it-q4_k_m

# í´ë°± ì„¤ì • ìœ ì§€
FALLBACK_AI_PROVIDER=openai
```

### ì—°ê²° í…ŒìŠ¤íŠ¸
```bash
# ì›¹ì•±ì—ì„œ ì±„íŒ… í…ŒìŠ¤íŠ¸
# ì½˜ì†”ì—ì„œ ì‘ë‹µ ì‹œê°„ í™•ì¸
# 1-3ì´ˆ ì´ë‚´ë©´ ì •ìƒ
```

---

## ğŸ‰ ì™„ë£Œ!
ì—°êµ¬ì‹¤ ì„œë²„ ì„¤ì •ì´ ì™„ë£Œë˜ë©´ 24/7 ì•ˆì •ì ì¸ AI ì„œë¹„ìŠ¤ ìš´ì˜ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.